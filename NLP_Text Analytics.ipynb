{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Natural Language Toolkit Library import\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data load\n",
    "data = \"India (Hindi: Bhārat), officially the Republic of India, is a country in South Asia. It is the seventh-largest country by area, the second-most populous country, and the most populous democracy in the world. Bounded by the Indian Ocean on the south, the Arabian Sea on the southwest, and the Bay of Bengal on the southeast, it shares land borders with Pakistan to the west; China, Nepal, and Bhutan to the north; and Bangladesh and Myanmar to the east. In the Indian Ocean, India is in the vicinity of Sri Lanka and the Maldives; its Andaman and Nicobar Islands share a maritime border with Thailand and Indonesia.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['India (Hindi: Bhārat), officially the Republic of India, is a country in South Asia.',\n",
       " 'It is the seventh-largest country by area, the second-most populous country, and the most populous democracy in the world.',\n",
       " 'Bounded by the Indian Ocean on the south, the Arabian Sea on the southwest, and the Bay of Bengal on the southeast, it shares land borders with Pakistan to the west; China, Nepal, and Bhutan to the north; and Bangladesh and Myanmar to the east.',\n",
       " 'In the Indian Ocean, India is in the vicinity of Sri Lanka and the Maldives; its Andaman and Nicobar Islands share a maritime border with Thailand and Indonesia.']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Tokenization using NLTK - Sentence tokenization\n",
    "nltk.sent_tokenize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['India',\n",
       " '(',\n",
       " 'Hindi',\n",
       " ':',\n",
       " 'Bhārat',\n",
       " ')',\n",
       " ',',\n",
       " 'officially',\n",
       " 'the',\n",
       " 'Republic',\n",
       " 'of',\n",
       " 'India',\n",
       " ',',\n",
       " 'is',\n",
       " 'a',\n",
       " 'country',\n",
       " 'in',\n",
       " 'South',\n",
       " 'Asia',\n",
       " '.',\n",
       " 'It',\n",
       " 'is',\n",
       " 'the',\n",
       " 'seventh-largest',\n",
       " 'country',\n",
       " 'by',\n",
       " 'area',\n",
       " ',',\n",
       " 'the',\n",
       " 'second-most',\n",
       " 'populous',\n",
       " 'country',\n",
       " ',',\n",
       " 'and',\n",
       " 'the',\n",
       " 'most',\n",
       " 'populous',\n",
       " 'democracy',\n",
       " 'in',\n",
       " 'the',\n",
       " 'world',\n",
       " '.',\n",
       " 'Bounded',\n",
       " 'by',\n",
       " 'the',\n",
       " 'Indian',\n",
       " 'Ocean',\n",
       " 'on',\n",
       " 'the',\n",
       " 'south',\n",
       " ',',\n",
       " 'the',\n",
       " 'Arabian',\n",
       " 'Sea',\n",
       " 'on',\n",
       " 'the',\n",
       " 'southwest',\n",
       " ',',\n",
       " 'and',\n",
       " 'the',\n",
       " 'Bay',\n",
       " 'of',\n",
       " 'Bengal',\n",
       " 'on',\n",
       " 'the',\n",
       " 'southeast',\n",
       " ',',\n",
       " 'it',\n",
       " 'shares',\n",
       " 'land',\n",
       " 'borders',\n",
       " 'with',\n",
       " 'Pakistan',\n",
       " 'to',\n",
       " 'the',\n",
       " 'west',\n",
       " ';',\n",
       " 'China',\n",
       " ',',\n",
       " 'Nepal',\n",
       " ',',\n",
       " 'and',\n",
       " 'Bhutan',\n",
       " 'to',\n",
       " 'the',\n",
       " 'north',\n",
       " ';',\n",
       " 'and',\n",
       " 'Bangladesh',\n",
       " 'and',\n",
       " 'Myanmar',\n",
       " 'to',\n",
       " 'the',\n",
       " 'east',\n",
       " '.',\n",
       " 'In',\n",
       " 'the',\n",
       " 'Indian',\n",
       " 'Ocean',\n",
       " ',',\n",
       " 'India',\n",
       " 'is',\n",
       " 'in',\n",
       " 'the',\n",
       " 'vicinity',\n",
       " 'of',\n",
       " 'Sri',\n",
       " 'Lanka',\n",
       " 'and',\n",
       " 'the',\n",
       " 'Maldives',\n",
       " ';',\n",
       " 'its',\n",
       " 'Andaman',\n",
       " 'and',\n",
       " 'Nicobar',\n",
       " 'Islands',\n",
       " 'share',\n",
       " 'a',\n",
       " 'maritime',\n",
       " 'border',\n",
       " 'with',\n",
       " 'Thailand',\n",
       " 'and',\n",
       " 'Indonesia',\n",
       " '.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word tokenization\n",
    "nltk.word_tokenize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('We', 'PRP'),\n",
       " ('will', 'MD'),\n",
       " ('see', 'VB'),\n",
       " ('an', 'DT'),\n",
       " ('example', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('POS', 'NNP'),\n",
       " ('tagging', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Part of speech\n",
    "data =' We will see an example of POS tagging.'\n",
    "\n",
    "pos = nltk.pos_tag(nltk.word_tokenize(data))\n",
    "\n",
    "pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  We/PRP\n",
      "  will/MD\n",
      "  see/VB\n",
      "  an/DT\n",
      "  (MN example/NN)\n",
      "  of/IN\n",
      "  (MN POS/NNP tagging/NN)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# Chunking-To categorise different words into one tag\n",
    "my_node = \"MN: {<NNP>*<NN>}\"\n",
    "\n",
    "chunk  =nltk.RegexpParser(my_node)\n",
    "result = chunk.parse(pos)\n",
    "print(result)\n",
    "result.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stop words\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Punctuations to ignore like stop words\n",
    "import string\n",
    "\n",
    "punct =string.punctuation\n",
    "punct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['India',\n",
       " 'Hindi',\n",
       " 'Bhārat',\n",
       " 'officially',\n",
       " 'Republic',\n",
       " 'India',\n",
       " 'country',\n",
       " 'South',\n",
       " 'Asia',\n",
       " 'It',\n",
       " 'seventh-largest',\n",
       " 'country',\n",
       " 'area',\n",
       " 'second-most',\n",
       " 'populous',\n",
       " 'country',\n",
       " 'populous',\n",
       " 'democracy',\n",
       " 'world',\n",
       " 'Bounded',\n",
       " 'Indian',\n",
       " 'Ocean',\n",
       " 'south',\n",
       " 'Arabian',\n",
       " 'Sea',\n",
       " 'southwest',\n",
       " 'Bay',\n",
       " 'Bengal',\n",
       " 'southeast',\n",
       " 'shares',\n",
       " 'land',\n",
       " 'borders',\n",
       " 'Pakistan',\n",
       " 'west',\n",
       " 'China',\n",
       " 'Nepal',\n",
       " 'Bhutan',\n",
       " 'north',\n",
       " 'Bangladesh',\n",
       " 'Myanmar',\n",
       " 'east',\n",
       " 'In',\n",
       " 'Indian',\n",
       " 'Ocean',\n",
       " 'India',\n",
       " 'vicinity',\n",
       " 'Sri',\n",
       " 'Lanka',\n",
       " 'Maldives',\n",
       " 'Andaman',\n",
       " 'Nicobar',\n",
       " 'Islands',\n",
       " 'share',\n",
       " 'maritime',\n",
       " 'border',\n",
       " 'Thailand',\n",
       " 'Indonesia']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word tokenization after removing the stopwords and punctuations\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "punct =string.punctuation\n",
    "\n",
    "data = \"India (Hindi: Bhārat), officially the Republic of India, is a country in South Asia. It is the seventh-largest country by area, the second-most populous country, and the most populous democracy in the world. Bounded by the Indian Ocean on the south, the Arabian Sea on the southwest, and the Bay of Bengal on the southeast, it shares land borders with Pakistan to the west; China, Nepal, and Bhutan to the north; and Bangladesh and Myanmar to the east. In the Indian Ocean, India is in the vicinity of Sri Lanka and the Maldives; its Andaman and Nicobar Islands share a maritime border with Thailand and Indonesia.\"\n",
    "clean_data =[]\n",
    "for word in nltk.word_tokenize(data):\n",
    "    if word not in punct:\n",
    "        if word not in stop_words:\n",
    "            clean_data.append(word)\n",
    "            \n",
    "clean_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('India', 'NNP'),\n",
       " ('Hindi', 'NNP'),\n",
       " ('Bhārat', 'NNP'),\n",
       " ('officially', 'RB'),\n",
       " ('Republic', 'NNP'),\n",
       " ('India', 'NNP'),\n",
       " ('country', 'NN'),\n",
       " ('South', 'NNP'),\n",
       " ('Asia', 'IN'),\n",
       " ('It', 'PRP'),\n",
       " ('seventh-largest', 'JJ'),\n",
       " ('country', 'NN'),\n",
       " ('area', 'NN'),\n",
       " ('second-most', 'RB'),\n",
       " ('populous', 'JJ'),\n",
       " ('country', 'NN'),\n",
       " ('populous', 'JJ'),\n",
       " ('democracy', 'NN'),\n",
       " ('world', 'NN'),\n",
       " ('Bounded', 'NNP'),\n",
       " ('Indian', 'JJ'),\n",
       " ('Ocean', 'NNP'),\n",
       " ('south', 'NN'),\n",
       " ('Arabian', 'NNP'),\n",
       " ('Sea', 'NNP'),\n",
       " ('southwest', 'JJS'),\n",
       " ('Bay', 'NNP'),\n",
       " ('Bengal', 'NNP'),\n",
       " ('southeast', 'NN'),\n",
       " ('shares', 'NNS'),\n",
       " ('land', 'VBP'),\n",
       " ('borders', 'NNS'),\n",
       " ('Pakistan', 'NNP'),\n",
       " ('west', 'JJS'),\n",
       " ('China', 'NNP'),\n",
       " ('Nepal', 'NNP'),\n",
       " ('Bhutan', 'NNP'),\n",
       " ('north', 'JJ'),\n",
       " ('Bangladesh', 'NNP'),\n",
       " ('Myanmar', 'NNP'),\n",
       " ('east', 'NN'),\n",
       " ('In', 'IN'),\n",
       " ('Indian', 'JJ'),\n",
       " ('Ocean', 'NNP'),\n",
       " ('India', 'NNP'),\n",
       " ('vicinity', 'NN'),\n",
       " ('Sri', 'NNP'),\n",
       " ('Lanka', 'NNP'),\n",
       " ('Maldives', 'NNP'),\n",
       " ('Andaman', 'NNP'),\n",
       " ('Nicobar', 'NNP'),\n",
       " ('Islands', 'NNP'),\n",
       " ('share', 'NN'),\n",
       " ('maritime', 'JJ'),\n",
       " ('border', 'NN'),\n",
       " ('Thailand', 'NNP'),\n",
       " ('Indonesia', 'NNP')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Part of speech tagging for the cleaned data\n",
    "nltk.pos_tag(clean_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Porter stemmer\n",
      "hobbi\n",
      "hobbi\n",
      "comput\n",
      "comput\n",
      "**************************\n",
      "lancaster stemmer\n",
      "hobby\n",
      "hobby\n",
      "comput\n",
      "comput\n",
      "**************************\n",
      "Snowball stemmer\n",
      "hobbi\n",
      "hobbi\n",
      "comput\n",
      "comput\n"
     ]
    }
   ],
   "source": [
    "# Stemming- bringing the words to their base form which may not be gramatically correct\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer, SnowballStemmer\n",
    "\n",
    "lancaster = LancasterStemmer()\n",
    "porter = PorterStemmer()\n",
    "Snowball = SnowballStemmer(\"english\")\n",
    "print('Porter stemmer')\n",
    "print(porter.stem(\"hobby\"))\n",
    "print(porter.stem(\"hobbies\"))\n",
    "print(porter.stem(\"computer\"))\n",
    "print(porter.stem(\"computation\"))\n",
    "print(\"**************************\")  \n",
    "print('lancaster stemmer')\n",
    "print(lancaster.stem(\"hobby\"))\n",
    "print(lancaster.stem(\"hobbies\"))\n",
    "print(lancaster.stem(\"computer\"))\n",
    "print(porter.stem(\"computation\"))\n",
    "print(\"**************************\")  \n",
    "print('Snowball stemmer')\n",
    "print(Snowball.stem(\"hobby\"))\n",
    "print(Snowball.stem(\"hobbies\"))\n",
    "print(Snowball.stem(\"computer\"))\n",
    "print(Snowball.stem(\"computation\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i was go to the offic on my bike when i saw a car pass by hit the tree .\n",
      "i was going to the off on my bik when i saw a car pass by hit the tre .\n",
      "I wa go to the offic on my bike when i saw a car pass by hit the tree .\n"
     ]
    }
   ],
   "source": [
    "# Using three of the stemming process together\n",
    "sent = \"I was going to the office on my bike when i saw a car passing by hit the tree.\"\n",
    "token = list(nltk.word_tokenize(sent))\n",
    "for stemmer in (Snowball, lancaster, porter):\n",
    "    stemm = [stemmer.stem(t) for t in token]\n",
    "    print(\" \".join(stemm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Lemmatization\n",
      "running\n",
      "run\n",
      "ran\n",
      "Using Stemming\n",
      "run\n",
      "run\n",
      "ran\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization - Unlike stemming, lemmetisation keeps the context of the base word in contact\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "print(\"Using Lemmatization\")\n",
    "print(lemma.lemmatize('running'))\n",
    "print(lemma.lemmatize('runs'))\n",
    "print(lemma.lemmatize('ran'))\n",
    "\n",
    "print(\"Using Stemming\")\n",
    "print(porter.stem(\"running\"))\n",
    "print(porter.stem(\"runs\"))\n",
    "print(porter.stem(\"ran\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n",
      "run\n",
      "run\n"
     ]
    }
   ],
   "source": [
    "# Giving part of speech to lemmatization\n",
    "print(lemma.lemmatize('running',pos='v'))\n",
    "print(lemma.lemmatize('runs',pos='v'))\n",
    "print(lemma.lemmatize('ran',pos='v'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (GPE India/NNP)\n",
      "  ,/,\n",
      "  officially/RB\n",
      "  the/DT\n",
      "  (ORGANIZATION Republic/NNP)\n",
      "  of/IN\n",
      "  (GPE India/NNP)\n",
      "  ,/,\n",
      "  is/VBZ\n",
      "  a/DT\n",
      "  country/NN\n",
      "  in/IN\n",
      "  (GPE South/NNP Asia/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "# Name Entity Recognition- One of the chunking methd possible to keep the group entities like people, country and regions together\n",
    "sent = \"India, officially the Republic of India, is a country in South Asia.\"\n",
    "\n",
    "words = nltk.word_tokenize(sent)\n",
    "pos_tag = nltk.pos_tag(words)\n",
    "namedEntity = nltk.ne_chunk(pos_tag)\n",
    "print(namedEntity)\n",
    "namedEntity.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\HP\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parsing - The process of determining the structure of a given sentence based on the basis of a given grammar rule\n",
    "grammar = nltk.CFG.fromstring(\"\"\"\n",
    "  S -> NP VP\n",
    "  VP -> V NP | V NP PP\n",
    "  PP -> P NP\n",
    "  V -> \"saw\" | \"slept\" | \"walked\"\n",
    "  NP -> \"Rahul\" | \"Anjali\" | Det N | Det N PP\n",
    "  Det -> \"a\" | \"an\" | \"the\" | \"my\"\n",
    "  N -> \"man\" | \"dog\" | \"cat\" | \"telescope\" | \"park\"\n",
    "  P -> \"in\" | \"on\" | \"by\" | \"with\"\n",
    "  \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP Rahul)\n",
      "  (VP (V saw) (NP Anjali) (PP (P with) (NP (Det a) (N dog)))))\n"
     ]
    }
   ],
   "source": [
    "sent = \"Rahul saw Anjali with a dog\".split()\n",
    "parser = nltk.RecursiveDescentParser(grammar)\n",
    "for tree in parser.parse(sent):\n",
    "    print(tree) \n",
    "    tree.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bag of words : ['an', 'bag', 'example', 'is', 'of', 'this', 'words']\n"
     ]
    }
   ],
   "source": [
    "# Word Vectorisation- It is the process of mapping the words to real number words to apply ome ML models in thhem to extract revevant information out of them.\n",
    "\n",
    "## Bag of words- The words are separated without giving any importance to the grammar.\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "string = [\"This is an example of bag of words!\"]\n",
    "vect1 = CountVectorizer()\n",
    "vect1.fit_transform(string)\n",
    "print(\"bag of words :\",vect1.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-gram  : ['an', 'example', 'gram', 'is', 'of', 'this']\n",
      "2-gram  : ['an example', 'example of', 'is an', 'of gram', 'this is']\n",
      "3-gram  : ['an example of', 'example of gram', 'is an example', 'this is an']\n",
      "4-gram  : ['an example of gram', 'is an example of', 'this is an example']\n"
     ]
    }
   ],
   "source": [
    "## n-gram model - Here the context of a given text is kept in touch\n",
    "# n-grams\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "string = [\"This is an example of n-gram!\"]\n",
    "vect1 = CountVectorizer(ngram_range=(1,1))\n",
    "vect1.fit_transform(string)\n",
    "vect2 = CountVectorizer(ngram_range=(2,2))\n",
    "vect2.fit_transform(string)\n",
    "vect3 = CountVectorizer(ngram_range=(3,3))\n",
    "vect3.fit_transform(string)\n",
    "vect4 = CountVectorizer(ngram_range=(4,4))\n",
    "vect4.fit_transform(string)\n",
    "print(\"1-gram  :\",vect1.get_feature_names())\n",
    "print(\"2-gram  :\",vect2.get_feature_names())\n",
    "print(\"3-gram  :\",vect3.get_feature_names())\n",
    "print(\"4-gram  :\",vect4.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>an</th>\n",
       "      <th>be</th>\n",
       "      <th>can</th>\n",
       "      <th>confusing</th>\n",
       "      <th>example</th>\n",
       "      <th>how</th>\n",
       "      <th>idf</th>\n",
       "      <th>is</th>\n",
       "      <th>it</th>\n",
       "      <th>see</th>\n",
       "      <th>this</th>\n",
       "      <th>we</th>\n",
       "      <th>will</th>\n",
       "      <th>works</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.408248</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.408248</td>\n",
       "      <td>0.408248</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.408248</td>\n",
       "      <td>0.408248</td>\n",
       "      <td>0.408248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    an   be  can  confusing  example       how  idf   is        it       see  \\\n",
       "0  0.5  0.0  0.0        0.0      0.5  0.000000  0.0  0.5  0.000000  0.000000   \n",
       "1  0.0  0.0  0.0        0.0      0.0  0.408248  0.0  0.0  0.408248  0.408248   \n",
       "2  0.0  0.5  0.5        0.5      0.0  0.000000  0.5  0.0  0.000000  0.000000   \n",
       "\n",
       "   this        we      will     works  \n",
       "0   0.5  0.000000  0.000000  0.000000  \n",
       "1   0.0  0.408248  0.408248  0.408248  \n",
       "2   0.0  0.000000  0.000000  0.000000  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tf-idf(Term frequency Inverse Frequency Document) - It does not give more importance to words with higher frequency\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "tfid = TfidfVectorizer(smooth_idf=False)\n",
    "\n",
    "doc= [\"This is an example.\",\"We will see how it works.\",\"IDF can be confusing\"]\n",
    "\n",
    "doc_vector = tfid.fit_transform(doc)\n",
    "df= pd.DataFrame(doc_vector.todense(),columns=tfid.get_feature_names())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>for</th>\n",
       "      <th>fun</th>\n",
       "      <th>has</th>\n",
       "      <th>idf</th>\n",
       "      <th>is</th>\n",
       "      <th>let</th>\n",
       "      <th>package</th>\n",
       "      <th>python</th>\n",
       "      <th>sklearn</th>\n",
       "      <th>tf</th>\n",
       "      <th>use</th>\n",
       "      <th>vectorization</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.408248</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.408248</td>\n",
       "      <td>0.408248</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.408248</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.408248</td>\n",
       "      <td>0.408248</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.57735</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.57735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        for      fun       has       idf       is      let   package   python  \\\n",
       "0  0.000000  0.00000  0.000000  0.000000  0.00000  0.57735  0.000000  0.57735   \n",
       "1  0.408248  0.00000  0.408248  0.408248  0.00000  0.00000  0.408248  0.00000   \n",
       "2  0.000000  0.57735  0.000000  0.000000  0.57735  0.00000  0.000000  0.00000   \n",
       "\n",
       "    sklearn        tf      use  vectorization  \n",
       "0  0.000000  0.000000  0.57735        0.00000  \n",
       "1  0.408248  0.408248  0.00000        0.00000  \n",
       "2  0.000000  0.000000  0.00000        0.57735  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "tfid = TfidfVectorizer()\n",
    "\n",
    "doc= [\"Let's use python!\", \"Sklearn has package for Tf-idf.\",\"Vectorization is fun!\"]\n",
    "\n",
    "doc_vector = tfid.fit_transform(doc)\n",
    "df= pd.DataFrame(doc_vector.todense(),columns=tfid.get_feature_names())\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
